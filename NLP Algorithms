{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ar1tr0/nlp-algorithms?scriptVersionId=231857394\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# NLP Algorithms\n\n© Aritro 'sortira' Shome","metadata":{}},{"cell_type":"markdown","source":"⚠️ here we use the `reuters` corpus provided by the Python `nltk` library as a real-world corpus","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\nnewsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\ncorpus = newsgroups.data[:10] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:40:52.838655Z","iopub.execute_input":"2025-04-04T13:40:52.839012Z","iopub.status.idle":"2025-04-04T13:40:54.417116Z","shell.execute_reply.started":"2025-04-04T13:40:52.838984Z","shell.execute_reply":"2025-04-04T13:40:54.416202Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Bag of Words\nLet the corpus consist of many sentences. Each sentences consist of certain words. We reduce all words to lower case strip all punctuation and then each sentence in its BoW representation consists of a _map_ where the _key_ is an unique word and the _value_ is the number of time it occurs in the sentence. BoW representation does _not_ preserve the order of the words in the sentence which leads to loss of information as far as meaning is concerned (_dog bites man_ and _man bites dog_ has the same BoW representation) but BoW is simple and fast enough for use cases such as document classification where frequencies of keywords matter more than their ordering. ","metadata":{}},{"cell_type":"code","source":"import string\nfrom typing import List, Dict\n\ndef bow(corpus: List[str]) -> List[Dict[str, int]]:\n    bow_list = []\n    translator = str.maketrans('', '', string.punctuation)\n    for sentence in corpus:\n        cleaned = sentence.lower().translate(translator)\n        words = cleaned.split()\n        word_freq = {}\n        for word in words:\n            word_freq[word] = word_freq.get(word, 0) + 1\n        bow_list.append(word_freq)\n    return bow_list\n\nbow_result = bow(corpus)\nfor i, doc in enumerate(bow_result):\n    print(f\"\\nTop 10 words in Document {i + 1}:\")\n    top_words = sorted(doc.items(), key=lambda x: x[1], reverse=True)[:10]\n    for word, freq in top_words:\n        print(f\"  {word:15s} -> {freq}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:43:25.388252Z","iopub.execute_input":"2025-04-04T13:43:25.388575Z","iopub.status.idle":"2025-04-04T13:43:25.501168Z","shell.execute_reply.started":"2025-04-04T13:43:25.388548Z","shell.execute_reply":"2025-04-04T13:43:25.499985Z"}},"outputs":[{"name":"stdout","text":"\nTop 10 words in Document 1:\n  the             -> 6\n  was             -> 4\n  this            -> 4\n  car             -> 4\n  i               -> 3\n  a               -> 3\n  if              -> 2\n  anyone          -> 2\n  on              -> 2\n  it              -> 2\n\nTop 10 words in Document 2:\n  the             -> 4\n  and             -> 3\n  a               -> 2\n  of              -> 2\n  their           -> 2\n  clock           -> 2\n  have            -> 2\n  experiences     -> 2\n  this            -> 2\n  poll            -> 2\n\nTop 10 words in Document 3:\n  the             -> 21\n  a               -> 12\n  i               -> 8\n  in              -> 5\n  to              -> 5\n  and             -> 5\n  of              -> 5\n  is              -> 5\n  180             -> 4\n  on              -> 4\n\nTop 10 words in Document 4:\n  do              -> 1\n  you             -> 1\n  have            -> 1\n  weiteks         -> 1\n  addressphone    -> 1\n  number          -> 1\n  id              -> 1\n  like            -> 1\n  to              -> 1\n  get             -> 1\n\nTop 10 words in Document 5:\n  the             -> 5\n  a               -> 2\n  that            -> 2\n  are             -> 2\n  bugs            -> 2\n  in              -> 2\n  warning         -> 2\n  they            -> 2\n  and             -> 2\n  from            -> 1\n\nTop 10 words in Document 6:\n  the             -> 4\n  to              -> 3\n  of              -> 2\n  term            -> 2\n  be              -> 2\n  i               -> 2\n  this            -> 2\n  that            -> 2\n  you             -> 2\n  as              -> 2\n\nTop 10 words in Document 7:\n  i               -> 4\n  a               -> 2\n  to              -> 2\n  for             -> 2\n  thank           -> 2\n  of              -> 2\n  there           -> 1\n  were            -> 1\n  few             -> 1\n  people          -> 1\n\nTop 10 words in Document 8:\n  is              -> 13\n  scsi1           -> 11\n  a               -> 8\n  with            -> 7\n  scsi2           -> 7\n  it              -> 7\n  scsi            -> 6\n  chip            -> 6\n  and             -> 6\n  the             -> 6\n\nTop 10 words in Document 9:\n  i               -> 2\n  and             -> 2\n  icons           -> 2\n  the             -> 2\n  have            -> 1\n  win             -> 1\n  30              -> 1\n  downloaded      -> 1\n  several         -> 1\n  bmps            -> 1\n\nTop 10 words in Document 10:\n  the             -> 14\n  board           -> 5\n  with            -> 4\n  to              -> 4\n  ive             -> 3\n  a               -> 3\n  and             -> 3\n  it              -> 3\n  but             -> 3\n  file            -> 3\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## TF-IDF Representation\n\nExtending the bag-of-words representation, we create something known as the **TF-IDF** (Term Frequency-Inverse Document Frequency) representation.\n\nIn simple words:\n\n- **TF** (Term Frequency) is a measure of how frequently a word occurs in a document.\n- **IDF** (Inverse Document Frequency) is a measure of how rare or informative a word is across the entire corpus.\n\nMathematically:\n\nLet:\n\n- $N$ be the total number of documents in the corpus.\n- $n_t$ be the number of documents in which the term $t$ appears.\n- $f_{t,d}$ be the frequency of term $t$ in document $d$.\n\nThen:\n\n$$\n\\text{TF}(t, d) = \\frac{f_{t,d}}{\\sum_k f_{k,d}}\n$$\n\n$$\n\\text{IDF}(t) = \\log\\left(\\frac{N}{1 + n_t}\\right)\n$$\n\n(Note: we add 1 to the denominator to avoid division by zero in case a word doesn’t appear in any document.)\n\n$$\n\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n$$\n\nThe TF-IDF representation reduces the impact of commonly occurring words like \"the\", \"is\", \"in\", etc., and highlights more meaningful or rare words in the context of the corpus. It is widely used in information retrieval, document classification, and text clustering tasks.\n","metadata":{}},{"cell_type":"code","source":"import math\nimport string\nfrom typing import List, Dict\ndef preprocess(text: str) -> List[str]:\n    translator = str.maketrans('', '', string.punctuation)\n    return text.lower().translate(translator).split()\ndef tf(t: str, d: List[str]) -> float:\n    return d.count(t) / len(d)\ndef idf(t: str, corpus: List[List[str]]) -> float:\n    N = len(corpus)\n    nt = sum(1 for doc in corpus if t in doc)\n    return math.log((1 + N) / (1 + nt))\ndef tf_idf_repr(corpus: List[str]) -> List[Dict[str, float]]:\n    tokenized_corpus = [preprocess(doc) for doc in corpus]\n    tf_idf_list = []\n    for doc in tokenized_corpus:\n        tf_idf_doc = {}\n        for word in set(doc):\n            tf_val = tf(word, doc)\n            idf_val = idf(word, tokenized_corpus)\n            tf_idf_doc[word] = tf_val * idf_val\n        tf_idf_list.append(tf_idf_doc)\n    return tf_idf_list\n\ntfidf_result = tf_idf_repr(corpus)\nfor i, doc in enumerate(tfidf_result):\n    print(f\"\\nTop 10 words in Document {i + 1}:\")\n    top_words = sorted(doc.items(), key=lambda x: x[1], reverse=True)[:10]\n    for word, score in top_words:\n        print(f\"  {word:15s} -> {score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T13:42:34.307025Z","iopub.execute_input":"2025-04-04T13:42:34.307373Z","iopub.status.idle":"2025-04-04T13:42:34.3547Z","shell.execute_reply.started":"2025-04-04T13:42:34.307348Z","shell.execute_reply":"2025-04-04T13:42:34.353681Z"}},"outputs":[{"name":"stdout","text":"\nTop 10 words in Document 1:\n  car             -> 0.0749\n  anyone          -> 0.0375\n  was             -> 0.0347\n  saw             -> 0.0187\n  engine          -> 0.0187\n  specs           -> 0.0187\n  where           -> 0.0187\n  name            -> 0.0187\n  rest            -> 0.0187\n  70s             -> 0.0187\n\nTop 10 words in Document 2:\n  add             -> 0.0379\n  their           -> 0.0379\n  experiences     -> 0.0379\n  clock           -> 0.0379\n  poll            -> 0.0379\n  speed           -> 0.0289\n  please          -> 0.0225\n  floppy          -> 0.0189\n  sinks           -> 0.0189\n  souls           -> 0.0189\n\nTop 10 words in Document 3:\n  180             -> 0.0231\n  heard           -> 0.0173\n  powerbook       -> 0.0173\n  anybody         -> 0.0173\n  display         -> 0.0173\n  the             -> 0.0143\n  could           -> 0.0132\n  bunch           -> 0.0116\n  somebody        -> 0.0116\n  around          -> 0.0116\n\nTop 10 words in Document 4:\n  addressphone    -> 0.1136\n  weiteks         -> 0.1136\n  information     -> 0.1136\n  do              -> 0.1136\n  like            -> 0.0866\n  number          -> 0.0866\n  chip            -> 0.0866\n  get             -> 0.0866\n  id              -> 0.0674\n  some            -> 0.0674\n\nTop 10 words in Document 5:\n  they            -> 0.0480\n  bugs            -> 0.0480\n  warning         -> 0.0480\n  introduce       -> 0.0240\n  code            -> 0.0240\n  suchlike        -> 0.0240\n  launch          -> 0.0240\n  possibly        -> 0.0240\n  till            -> 0.0240\n  crew            -> 0.0240\n\nTop 10 words in Document 6:\n  term            -> 0.0416\n  first           -> 0.0416\n  weapons         -> 0.0416\n  then            -> 0.0317\n  as              -> 0.0247\n  evidently       -> 0.0208\n  her             -> 0.0208\n  analysis        -> 0.0208\n  destruction     -> 0.0208\n  topics          -> 0.0208\n\nTop 10 words in Document 7:\n  thank           -> 0.0541\n  mailbouncing    -> 0.0271\n  publicly        -> 0.0271\n  sure            -> 0.0271\n  sean            -> 0.0271\n  delete          -> 0.0271\n  sharon          -> 0.0271\n  few             -> 0.0271\n  september       -> 0.0271\n  instead         -> 0.0271\n\nTop 10 words in Document 8:\n  scsi1           -> 0.0713\n  scsi2           -> 0.0454\n  scsi            -> 0.0389\n  burst           -> 0.0324\n  is              -> 0.0300\n  chip            -> 0.0296\n  10mbs           -> 0.0259\n  fast            -> 0.0259\n  with            -> 0.0210\n  which           -> 0.0198\n\nTop 10 words in Document 9:\n  icons           -> 0.0838\n  appreciated     -> 0.0550\n  figure          -> 0.0550\n  help            -> 0.0550\n  cant            -> 0.0550\n  bmps            -> 0.0550\n  brando          -> 0.0550\n  win             -> 0.0550\n  change          -> 0.0550\n  wallpaper       -> 0.0550\n\nTop 10 words in Document 10:\n  board           -> 0.0543\n  its             -> 0.0248\n  file            -> 0.0248\n  ive             -> 0.0248\n  lost            -> 0.0217\n  being           -> 0.0217\n  boards          -> 0.0217\n  stac            -> 0.0217\n  product         -> 0.0217\n  licensing       -> 0.0217\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"Now so far, the word embeddings we have used are _frequency based_. They are fast, suitable for many use cases, but for the machines to understand the semantics of speech, such embeddings shall not suffice as they fail to capture the meaning behind the words (e.g. has no information about order, etc.)","metadata":{}},{"cell_type":"markdown","source":"## Word2Vec: Continuous Bag of Words (CBOW)\n\nThe **Continuous Bag of Words (CBOW)** model is a type of Word2Vec architecture used for learning **word embeddings**. The idea is simple: **given a context (surrounding words), predict the center (target) word**.\n\n---\n\n### 🧠 Intuition\n\nGiven a window of context words (e.g., 2 words before and after), CBOW tries to **guess the missing word** in the middle.\n\nFor example:\n\nContext: [\"the\", \"cat\", \"on\", \"the\"] → Target: \"sat\"\n\n\n\n---\n\n### 🔢 Step-by-Step Workflow\n\n1. **Input**: Context words (e.g., 4 words around the center)\n2. **One-hot encode** each context word (vector of size V, the vocab size)\n3. **Embedding Layer**: Map one-hot to dense vectors using the embedding matrix E\n4. **Average** the embeddings of the context words\n5. **Output Layer**: Use a weight matrix W' to produce scores over vocab\n6. **Softmax**: Convert scores to probabilities\n7. **Loss**: Use cross-entropy loss to compare predicted vs actual center word\n8. **Training**: Backpropagate error and update E and W'\n\n---\n\n### 🔬 Mathematical Formulation\n\nLet:\n- Vocabulary size be V\n- Embedding dimension be D\n- Context size be C (number of context words)\n- One-hot vector for word w_i: \\( x_i \\in \\mathbb{R}^V \\)\n- Embedding matrix: \\( E \\in \\mathbb{R}^{V \\times D} \\)\n\n---\n\n### ⛓ Equations\n\n**Embedding Lookup:**\n$$\nh = \\frac{1}{C} \\sum_{i=1}^{C} E^\\top x_i \\in \\mathbb{R}^D\n$$\n\n**Output scores:**\n$$\nz = W' h + b \\in \\mathbb{R}^V\n$$\n\n**Softmax prediction:**\n$$\n\\hat{y} = \\text{softmax}(z) = \\frac{e^{z_j}}{\\sum_{k=1}^V e^{z_k}}\n$$\n\n**Loss (Cross Entropy):**\n$$\n\\mathcal{L} = -\\log(\\hat{y}_{t})\n$$\nwhere \\( t \\) is the index of the true center word.\n\n---\n\n### 🧪 Learning\n\nDuring training, the weights in the embedding matrix E are updated so that **words appearing in similar contexts get similar vector representations**.\n\nThis leads to rich and meaningful **word embeddings** that capture both **semantic** and **syntactic** relationships between words.\n\n---\n\n### 🗃️ Example\n\nContext: [\"deep\", \"learning\", \"is\", \"fun\"] → Target: \"really\"\n\nModel predicts \"really\" using embeddings of the context words. Gradually, embeddings for \"fun\", \"deep\", \"learning\" will cluster together in vector space.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import fetch_20newsgroups\nimport re\nimport string\nfrom collections import Counter\nimport random\nnewsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\ncorpus_raw = newsgroups.data[:500] \ndef preprocess(text):\n    text = text.lower()\n    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n    return text.split()\ncorpus = [preprocess(doc) for doc in corpus_raw]\nflattened = [word for sentence in corpus for word in sentence]\nvocab_size = 5000\nword_counts = Counter(flattened)\nmost_common = word_counts.most_common(vocab_size - 1)\nword_to_ix = {word: i+1 for i, (word, _) in enumerate(most_common)}\nword_to_ix[\"<UNK>\"] = 0\nix_to_word = {i: word for word, i in word_to_ix.items()}\n\ndef get_index(word):\n    return word_to_ix.get(word, word_to_ix[\"<UNK>\"])\ncontext_window = 2\ndata = []\nfor sentence in corpus:\n    indices = [get_index(word) for word in sentence]\n    for i in range(context_window, len(indices) - context_window):\n        context = indices[i - context_window:i] + indices[i+1:i+context_window+1]\n        target = indices[i]\n        data.append((context, target))\nclass CBOWDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = data\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        context, target = self.data[idx]\n        return torch.tensor(context), torch.tensor(target)\ntrain_loader = torch.utils.data.DataLoader(CBOWDataset(data), batch_size=128, shuffle=True)\nclass CBOWModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(CBOWModel, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n    \n    def forward(self, context):\n        embeds = self.embeddings(context)  # (batch_size, context_size, embedding_dim)\n        avg_embeds = embeds.mean(dim=1)   # (batch_size, embedding_dim)\n        out = self.linear1(avg_embeds)    # (batch_size, vocab_size)\n        return out\nembedding_dim = 100\nmodel = CBOWModel(vocab_size, embedding_dim)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\nfor epoch in range(100):\n    total_loss = 0\n    for context, target in train_loader:\n        optimizer.zero_grad()\n        output = model(context)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n\ndef predict(context_words):\n    context_indices = [get_index(w) for w in context_words]\n    context_tensor = torch.tensor([context_indices])\n    with torch.no_grad():\n        output = model(context_tensor)\n        predicted_idx = torch.argmax(output, dim=1).item()\n        return ix_to_word.get(predicted_idx, \"<UNK>\")\n\n\nsample_word = \"money\"\nword_idx = torch.tensor([get_index(sample_word)])\nembedding = model.embeddings(word_idx).detach().numpy()\nprint(f\"\\n🧬 Embedding for '{sample_word}':\\n{embedding}\")\n\nprint(\"\\n🔮 Predictions from sample context:\")\nsample_contexts = [\n    [\"stock\", \"is\", \"volatile\"],\n    [\"intelligence\", \"is\", \"changing\"],\n    [\"york\", \"is\", \"crowded\"],\n    [\"team\", \"played\", \"very\"],\n    [\"the\", \"economy\", \"is\"]\n]\n\ndef clean(word):\n    return word.lower().translate(str.maketrans('', '', string.punctuation))\n\nfor context_words in sample_contexts:\n    context_words = [clean(w) for w in context_words]\n    prediction = predict(context_words)\n    print(f\"Context: {context_words} -> Predicted center word: '{prediction}'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:06:48.029643Z","iopub.execute_input":"2025-04-04T14:06:48.030122Z","iopub.status.idle":"2025-04-04T14:18:58.469771Z","shell.execute_reply.started":"2025-04-04T14:06:48.03008Z","shell.execute_reply":"2025-04-04T14:18:58.468723Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 3666.6421\nEpoch 2, Loss: 3036.8304\nEpoch 3, Loss: 2722.6312\nEpoch 4, Loss: 2467.5086\nEpoch 5, Loss: 2251.1653\nEpoch 6, Loss: 2066.0783\nEpoch 7, Loss: 1908.4221\nEpoch 8, Loss: 1772.3043\nEpoch 9, Loss: 1654.9003\nEpoch 10, Loss: 1551.4603\nEpoch 11, Loss: 1460.2071\nEpoch 12, Loss: 1379.3459\nEpoch 13, Loss: 1307.1037\nEpoch 14, Loss: 1241.2798\nEpoch 15, Loss: 1182.6164\nEpoch 16, Loss: 1128.9669\nEpoch 17, Loss: 1081.5004\nEpoch 18, Loss: 1037.4983\nEpoch 19, Loss: 997.4545\nEpoch 20, Loss: 960.7027\nEpoch 21, Loss: 927.1091\nEpoch 22, Loss: 896.8735\nEpoch 23, Loss: 869.2475\nEpoch 24, Loss: 843.3121\nEpoch 25, Loss: 819.3885\nEpoch 26, Loss: 797.8498\nEpoch 27, Loss: 777.4871\nEpoch 28, Loss: 759.3788\nEpoch 29, Loss: 742.3749\nEpoch 30, Loss: 726.2313\nEpoch 31, Loss: 711.7481\nEpoch 32, Loss: 697.4957\nEpoch 33, Loss: 685.3379\nEpoch 34, Loss: 673.0159\nEpoch 35, Loss: 661.6436\nEpoch 36, Loss: 651.2125\nEpoch 37, Loss: 641.6606\nEpoch 38, Loss: 632.2460\nEpoch 39, Loss: 623.2841\nEpoch 40, Loss: 615.0347\nEpoch 41, Loss: 607.6091\nEpoch 42, Loss: 599.8343\nEpoch 43, Loss: 593.7974\nEpoch 44, Loss: 586.4772\nEpoch 45, Loss: 580.7275\nEpoch 46, Loss: 574.5918\nEpoch 47, Loss: 569.2060\nEpoch 48, Loss: 563.3648\nEpoch 49, Loss: 558.2849\nEpoch 50, Loss: 554.1814\nEpoch 51, Loss: 548.8604\nEpoch 52, Loss: 544.6288\nEpoch 53, Loss: 540.5744\nEpoch 54, Loss: 536.2060\nEpoch 55, Loss: 532.4317\nEpoch 56, Loss: 528.8388\nEpoch 57, Loss: 524.9914\nEpoch 58, Loss: 521.9903\nEpoch 59, Loss: 518.1050\nEpoch 60, Loss: 515.1010\nEpoch 61, Loss: 511.7788\nEpoch 62, Loss: 509.3827\nEpoch 63, Loss: 506.6273\nEpoch 64, Loss: 504.0032\nEpoch 65, Loss: 501.2980\nEpoch 66, Loss: 499.0740\nEpoch 67, Loss: 496.1860\nEpoch 68, Loss: 493.5589\nEpoch 69, Loss: 491.3630\nEpoch 70, Loss: 489.6047\nEpoch 71, Loss: 487.3558\nEpoch 72, Loss: 485.2875\nEpoch 73, Loss: 483.1762\nEpoch 74, Loss: 481.4279\nEpoch 75, Loss: 479.0217\nEpoch 76, Loss: 477.6523\nEpoch 77, Loss: 475.8878\nEpoch 78, Loss: 474.1531\nEpoch 79, Loss: 472.3206\nEpoch 80, Loss: 470.4012\nEpoch 81, Loss: 469.1663\nEpoch 82, Loss: 467.8067\nEpoch 83, Loss: 466.2152\nEpoch 84, Loss: 465.5057\nEpoch 85, Loss: 463.5988\nEpoch 86, Loss: 462.1513\nEpoch 87, Loss: 460.8856\nEpoch 88, Loss: 459.6496\nEpoch 89, Loss: 458.0535\nEpoch 90, Loss: 457.4581\nEpoch 91, Loss: 455.1149\nEpoch 92, Loss: 454.7766\nEpoch 93, Loss: 453.6642\nEpoch 94, Loss: 452.7904\nEpoch 95, Loss: 451.3624\nEpoch 96, Loss: 450.1580\nEpoch 97, Loss: 449.4306\nEpoch 98, Loss: 448.5549\nEpoch 99, Loss: 447.1912\nEpoch 100, Loss: 446.4385\n\n🧬 Embedding for 'money':\n[[ 1.35192549e+00  4.45720822e-01  1.81187558e+00  1.57513404e+00\n   6.87746465e-01 -3.36645675e+00 -1.16775386e-01 -6.59322166e+00\n   3.01733792e-01 -6.22496188e-01  1.17816126e+00  2.12493515e+00\n   9.51652825e-01  1.12841137e-01  6.96030378e-01 -1.70881197e-01\n  -9.57327604e-01 -1.57548141e+00  2.71242809e+00 -1.56164491e+00\n   7.74970114e-01 -7.90690601e-01 -6.56922340e-01 -4.64641660e-01\n   2.57230163e+00  2.53789258e+00 -2.12438792e-01 -1.24830747e+00\n   6.29775584e-01 -1.84270307e-01 -3.48725152e+00 -6.19198620e-01\n   1.85693368e-01 -2.04454184e+00 -1.75403607e+00 -2.54308999e-01\n  -1.40875220e+00  2.64392376e+00  2.72570729e+00 -1.42279887e+00\n   1.28809798e+00 -2.57833314e+00 -7.02488959e-01 -1.99448884e+00\n   8.90658021e-01  4.68628049e-01  1.20475340e+00 -2.07781053e+00\n   6.16982579e-01 -4.07250452e+00 -1.64841473e+00 -1.74060810e+00\n  -1.95953393e+00 -1.25199020e+00 -1.33824766e+00 -3.88251036e-01\n  -5.17855406e-01  1.32933533e+00  1.52178466e+00 -8.69990706e-01\n  -5.73729217e-01  7.98604846e-01 -3.61698866e+00  4.68321979e-01\n   5.71179807e-01 -3.18878913e+00 -2.10868359e+00 -2.47999096e+00\n  -2.88412166e+00 -2.13368082e+00  7.87267506e-01  6.84750438e-01\n   1.39957881e+00  2.21994281e+00  1.28271237e-01 -2.22107840e+00\n   3.86856675e+00 -8.70942101e-02 -1.22002582e-03  1.50640094e+00\n   8.36382091e-01 -6.35101736e-01  1.99405581e-01 -4.27506161e+00\n   9.90230739e-01  3.48461533e+00 -5.40406168e-01  1.18868494e+00\n  -5.64181387e-01 -2.75526732e-01 -1.30682659e+00  3.73705655e-01\n  -6.94065332e-01  7.65791476e-01  8.88345778e-01 -1.04056430e+00\n  -4.02604008e+00  3.03538537e+00 -1.57912087e+00  6.13566041e-01]]\n\n🔮 Predictions from sample context:\nContext: ['stock', 'is', 'volatile'] -> Predicted center word: '<UNK>'\nContext: ['intelligence', 'is', 'changing'] -> Predicted center word: 'machine'\nContext: ['york', 'is', 'crowded'] -> Predicted center word: 'new'\nContext: ['team', 'played', 'very'] -> Predicted center word: 'well'\nContext: ['the', 'economy', 'is'] -> Predicted center word: 'likely'\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Byte-Pair Encoding Tokenizer\nFirst we build a dictionary of the corpus where the key is the word (converted into a list with a special </w> character to denote the end of the word) and the value is its frequency. Then what we do is a bigram analysis where we iterate over all words, find all adjacent character pairs and store them alongside their frequencies. Then we go over our input word, analyse the pairs, look at the adjacent character pairs analysed earlier and if they are present enough, we merge them and update our vocabulary with the merged item to be treated as one instead of 2. Then we keep doing it for the next and next and whenever we find a new element that can be merged, it is merged into the single element and vocabulary is updated (a simple way to visualize it is the way amoeba eats food using pseudopodium, that way the similar element gobbles up). ","metadata":{}},{"cell_type":"code","source":"import collections\n\ndef get_vocab(corpus):\n    vocab = collections.defaultdict(int)\n    for word in corpus:\n        symbols = list(word) + ['</w>']\n        vocab[tuple(symbols)] += 1\n    return vocab\n\ndef get_stats(vocab):\n    stats = collections.defaultdict(int)\n    for word, freq in vocab.items():\n        for i in range(len(word) - 1):\n            pair = (word[i], word[i + 1])\n            stats[pair] += freq\n    return stats\n\ndef merge_vocab(pair, vocab):\n    new_vocab = {}\n    bigram = \"\".join(pair)\n    for word, freq in vocab.items():\n        new_word = []\n        i = 0\n        while i < len(word):\n            if i < len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(bigram)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        new_vocab[tuple(new_word)] = freq\n    return new_vocab\n\ndef train_bpe(corpus, num_merges):\n    vocab = get_vocab(corpus)\n    merges = []\n    for _ in range(num_merges):\n        stats = get_stats(vocab)\n        if not stats:\n            break\n        best_pair = max(stats, key=stats.get)\n        if stats[best_pair] < 2:\n            break\n        vocab = merge_vocab(best_pair, vocab)\n        merges.append(best_pair)\n    return merges\n\ndef apply_bpe(text, merges):\n    word = list(text) + ['</w>']\n    for pair in merges:\n        bigram = \"\".join(pair)\n        new_word = []\n        i = 0\n        while i < len(word):\n            if i < len(word) - 1 and (word[i], word[i + 1]) == pair:\n                new_word.append(bigram)\n                i += 2\n            else:\n                new_word.append(word[i])\n                i += 1\n        word = new_word\n    return word[:-1]\n\ntestcorpus = [\"hello\", \"help\", \"hell\", \"helmet\"]\nnum_merges = 5\nmerges = train_bpe(testcorpus, num_merges)\n\ntokenized = apply_bpe(\"hello\", merges)\nprint(\"Tokenized Output:\", tokenized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T14:23:00.707442Z","iopub.execute_input":"2025-04-04T14:23:00.707835Z","iopub.status.idle":"2025-04-04T14:23:00.720981Z","shell.execute_reply.started":"2025-04-04T14:23:00.707803Z","shell.execute_reply":"2025-04-04T14:23:00.71948Z"}},"outputs":[{"name":"stdout","text":"Tokenized Output: ['hell', 'o']\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"## Word2Vec: Skip-gram Model (with Backpropagation)\n\nThe **Skip-gram** model learns word embeddings by predicting context words given a center word. Unlike CBOW, which averages over context, Skip-gram optimizes **multiple outputs per input**.\n\n---\n\n### 🧠 Recap\n\n- **Input**: center word  \n- **Output**: each word in the context window  \n- **Objective**: Maximize the probability of each context word given the center word\n\n---\n\n### 🔄 Forward Pass\n\n#### 📌 Notation\n\n- $V$: Vocabulary size\n- $D$: Embedding dimension\n- $C$: Number of context words (on each side)\n- $x \\in $\\mathbb{R}^V$: One-hot vector of the center word\n- $E \\in \\mathbb{R}^{V \\times D}$ : Embedding matrix\n- $W' \\in \\mathbb{R}^{D \\times V}$: Output weight matrix\n\n---\n\n### 🔢 Step-by-step\n\n#### **1. Embedding lookup**\nCenter word $w_c$:\n\n$\nh = E^T x \\in \\mathbb{R}^D\n$\n\nThis selects row $E[w_c]$\n\n#### **2. Output score**\nEach context word score:\n\n$\nz = {W'}^T h \\in \\mathbb{R}^V\n$\n\n#### **3. Softmax**\nPredicted probability for word $w_o$:\n\n$\n\\hat{y}_{o} = \\frac{e^{z_o}}{\\sum_{j=1}^{V} e^{z_j}}\n$\n\n---\n\n### 🎯 Loss Function\n\nIf $w_o$ is the true context word, the loss is:\n\n$\n\\mathcal{L} = -\\log \\hat{y}_{o} = -z_o + \\log \\sum_{j=1}^V e^{z_j}\n$\n\n---\n\n### 🔁 Backpropagation\n\nWe now compute the **gradients** with respect to model parameters.\n\n### 💡 Gradient w.r.t. Output Weights $W'$\n\nFor each output word $w_o$:\n\n#### Derivative of loss w.r.t. scores:\n\n$\n\\frac{\\partial \\mathcal{L}}{\\partial z_j} = \\hat{y}_j - y_j\n$\n\nWhere $y_j = 1$ if $j = o$, else 0.\n\n#### Chain rule: Gradient w.r.t. $W'$:\n\nLet $z = W'^T h \\Rightarrow z_j = {w'_j}^T h$\n\nSo:\n\n$\n\\frac{\\partial \\mathcal{L}}{\\partial W'} = h (\\hat{y} - y)^T\n$\n\n---\n\n### 💡 Gradient w.r.t. Hidden Layer (center word embedding $h$)\n\nUsing:\n\n$\n\\frac{\\partial \\mathcal{L}}{\\partial h} = W' (\\hat{y} - y)\n$\n\n---\n\n### 💡 Gradient w.r.t. Embedding Matrix $E$\n\nSince $h = E^T.x$, we only update the row corresponding to the center word $w_c$:\n\n$\n\\frac{\\partial \\mathcal{L}}{\\partial E[w_c]} = \\frac{\\partial \\mathcal{L}}{\\partial h}\n$\n\n---\n\n### 🔄 Weight Updates (Gradient Descent)\n\nUsing learning rate $\\eta$:\n\n- Update output matrix:\n\n$\nW' \\leftarrow W' - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W'}\n$\n\n- Update embedding matrix:\n\n$\nE[w_c] \\leftarrow E[w_c] - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial E[w_c]}\n$\n\n---\n\n### 📚 Final Notes\n\n- **Skip-gram trains on multiple context words**, so the full loss is summed or averaged over all context targets.\n- **Negative sampling** or **hierarchical softmax** is often used to make the softmax more efficient for large vocabularies.\n\n---\n\n### 🧠 Intuition Summary\n\n- The **embedding matrix $E$** stores vector representations for input words.\n- The **output matrix $W'$** helps learn which embeddings should predict which output words.\n- Training encourages the dot product $h^T.w'_o$ to be **high for real context words** ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.datasets import fetch_20newsgroups\nimport re, string\nfrom collections import Counter\nimport math\n\nnewsgroups = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\ncorpus_raw = newsgroups.data[:1000]\n\ndef preprocess(text):\n    text = text.lower()\n    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n    return text.split()\n\ncorpus = [preprocess(doc) for doc in corpus_raw]\nflattened = [word for sentence in corpus for word in sentence]\n\nvocab_size = 5000\nword_counts = Counter(flattened)\nmost_common = word_counts.most_common(vocab_size - 1)\nword_to_ix = {word: i+1 for i, (word, _) in enumerate(most_common)}\nword_to_ix[\"<UNK>\"] = 0\nix_to_word = {i: word for word, i in word_to_ix.items()}\n\ndef get_index(word):\n    return word_to_ix.get(word, 0)\n\ncontext_window = 2\ndata = []\nfor sentence in corpus:\n    indices = [get_index(w) for w in sentence]\n    for i in range(len(indices)):\n        for j in range(-context_window, context_window + 1):\n            if j == 0 or i + j < 0 or i + j >= len(indices):\n                continue\n            context = indices[i + j]\n            target = indices[i]\n            data.append((target, context))\n\nclass SkipGramDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.data = data\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, idx):\n        target, context = self.data[idx]\n        return torch.tensor(target), torch.tensor(context)\n\ntrain_loader = torch.utils.data.DataLoader(SkipGramDataset(data), batch_size=128, shuffle=True)\n\nclass SkipGramModel(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(SkipGramModel, self).__init__()\n        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n\n    def forward(self, target_word):\n        target_embedding = self.in_embed(target_word)  # (batch_size, embed_dim)\n        output_embedding = self.out_embed.weight        # (vocab_size, embed_dim)\n        scores = torch.matmul(target_embedding, output_embedding.t())  # (batch_size, vocab_size)\n        return scores\n\nembedding_dim = 100\nmodel = SkipGramModel(vocab_size, embedding_dim)\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\n\nfor epoch in range(10):\n    total_loss = 0\n    for target, context in train_loader:\n        optimizer.zero_grad()\n        output = model(target)\n        loss = loss_fn(output, context)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n\ndef cosine_similarity(vec1, vec2):\n    return torch.dot(vec1, vec2) / (vec1.norm() * vec2.norm())\n\ndef angle_between(vec1, vec2):\n    cos_sim = cosine_similarity(vec1, vec2).item()\n    cos_sim = max(min(cos_sim, 1.0), -1.0)\n    return math.degrees(math.acos(cos_sim))\n\ndef show_similarity(word1, word2):\n    idx1, idx2 = get_index(word1), get_index(word2)\n    emb1 = model.in_embed(torch.tensor(idx1)).detach()\n    emb2 = model.in_embed(torch.tensor(idx2)).detach()\n    cos_sim = cosine_similarity(emb1, emb2).item()\n    angle = angle_between(emb1, emb2)\n    print(f\"\\n📏 Cosine similarity between '{word1}' and '{word2}': {cos_sim:.4f}\")\n    print(f\"🎯 Angle between '{word1}' and '{word2}' in embedding space: {angle:.2f}°\")\n\nshow_similarity(\"money\", \"cash\")\nshow_similarity(\"computer\", \"science\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}